Neural Network - Classification going to Neural Network return result

Neural Network consist of Layers 

Input layer = raw data == for thesis (available picks + banned hero = drafting available heroes)
Output layer = result or classification that we want == for thesis (top heroes recommendation (1-5)
Hidden layer = input layer connects in hidden layer by weights 

if Input layer connects with Hidden layer or Hidden layer connects with Output Layer in different inputs its called densely connected layer

Bias Node weight = 1
Biases are typically added at each layer (except input)

(Output result is based on the input layer data)

Activation functions (like ReLU, sigmoid, tanh) are non-linear functions applied after the weighted sum

Loss Function compares the entire network's output against the true labels

Gradient Descent is the algorithm used to find the optimal parameters of weight and biases it also can visualize lowest loss output

Backpropagation calculates the gradient of the loss function with respect to each weight in the network

In Neural Network the more information/raw data you have the better the network would be. Because in Neural Network the layers has been training simultaneously. After the whole process of neural network the backpropagation is the one that has been very helpful to help other layers by its algorithm.

Optimizer Function implements the gradient descent and the backpropagation algorithm

Data needs to be one-hot encoded 